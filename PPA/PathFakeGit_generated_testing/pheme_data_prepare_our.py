from preprocess4wae import *
import re


def pad_zero(lst, max_len=-1):
    if max_len == -1:
        for i in lst:
            max_len = max(max_len, len(i))
    for cnt in range(len(lst)):
        lst[cnt] = lst[cnt] + [0] * (max_len - len(lst[cnt]))
        # æˆªæ–­è¿‡é•¿çš„éƒ¨åˆ†ï¼›å½“max-lenæ˜¯æ‰‹åŠ¨è®¾ç½®çš„å‚æ•°æ—¶ï¼Œéœ€è¦æˆªæ–­é•¿åº¦è¶…è¿‡çš„éƒ¨åˆ†
        lst[cnt] = lst[cnt][:max_len]
    return lst


def emoji_replace(text):
    emoji_replace_dict = {"ğŸ˜‚": " emoji_a ", "ğŸ˜³": " emoji_b ", "ğŸ‘€": " emoji_c ",
                          "ğŸ™": " emoji_d ", "â¤": " emoji_e ", "ğŸ‘": " emoji_f ",
                          "ğŸ™Œ": " emoji_g ", "ğŸ˜­": " emoji_h ", "ğŸ˜’": " emoji_i ",
                          "ğŸ˜©": " emoji_j ", "ğŸ˜·": " emoji_k ", "ğŸ‘": " emoji_l ",
                          "ğŸ˜": " emoji_m ", "ğŸ‰": " emoji_n ", "ğŸ˜«": " emoji_o ",
                          "ğŸ˜”": " emoji_p ", "ğŸ’”": " emoji_q ", "ğŸ˜Š": " emoji_r ",
                          "ğŸ˜": " emoji_s ", "ğŸ™…": " emoji_t "}
    for k in emoji_replace_dict.keys():
        if k in text:
            text = text.replace(k, emoji_replace_dict[k])
    return text


def delete_special_freq(text):
    if len(text) < 20: return text
    # print('è½¬æ¢å‰ï¼š', text)
    raw = text
    text = [word for word in text.split() if word[0] != '@']
    text = ' '.join(text)
    # å¦‚æœåªæœ‰@ï¼Œåˆ™ä¿ç•™@
    if text == '': text = raw
    text = text.replace('URL', '')
    # å¦‚æœåªæœ‰urlï¼Œåˆ™ä¿ç•™url
    if text == '': text = raw
    # print('è½¬æ¢åï¼š', text)
    # print()
    return text


def replace_url_with_token(url):
    # print("æ›¿æ¢å‰ï¼š", url)
    url = re.sub(r'(https|http)?:\/\/(\w|\.|\/|\?|\=|\&|\%)*\b', 'URL', url, flags=re.MULTILINE)
    # print("æ›¿æ¢åï¼š", url)
    return url


# è¯»å–å…¨éƒ¨å›å¤æ–‡æœ¬ï¼ˆ1000000æ˜¯ä¸Šé™ï¼›å¦‚æœæ–‡ä»¶ä¸­è¶…è¿‡ä¸Šé™ï¼Œåˆ™åé¢çš„ä¸å¤„ç†ï¼Œç”¨äºå¤„ç†æ•°æ®è¿‡å¤šçš„æ–‡ä»¶ï¼Œ100000æ‰‹åŠ¨è®¾ç½®ï¼‰
raw_tokenized_all_examples = load_data(100000, 'data/pathPheme5_texts.txt', True, "spacy")
examples_num = len(raw_tokenized_all_examples)
# æŒ‰ç…§æ¯”ä¾‹åˆ’åˆ†ä½œä¸ºè®­ç»ƒå’ŒéªŒè¯é›†
# å¤åˆ¶ä¸€éï¼›ä¾¿äºæ‰“ä¹±é¡ºåºç­‰æ“ä½œ
random_all_examples = [line for line in raw_tokenized_all_examples]
random.shuffle(random_all_examples)
print("fitting count vectorizer...")

count_vectorizer = CountVectorizer(stop_words='english',
                                   max_features=5000,
                                   token_pattern=r'\b[^\d\W]{2,20}\b'
                                   )
# tfidf_vectorizer = TfidfVectorizer(stop_words='english',
#                                    max_features=args.vocab_size,
#                                    token_pattern=r'\b[^\d\W]{3,30}\b')

count_vectorizer.fit(tqdm(random_all_examples))

nlp = spacy.load('en_core_web_sm')
tokenizer = Tokenizer(nlp.vocab)

response_id2paths_dict = {}
random_id2paths_dict = {}
data_path = '220913__new_data/gen_test.json'
with tqdm(open(data_path, "r"), desc=f"loading {data_path}") as f:
    for line in f:
        example = json.loads(line.strip())
        tokenized_examples = []
        id_ = example['id_']
        response_tweets = example['tweets'][1:]
        for i in response_tweets:
            i = replace_url_with_token(i)
            i = emoji_replace(i)
            i = delete_special_freq(i)
            tokens = list(map(str, tokenizer(i)))
            text = ' '.join(tokens)
            tokenized_examples.append(text)

        vectorized_raw_all_examples = count_vectorizer.transform(tqdm(tokenized_examples))

        vectorized_raw_all_examples = sparse.hstack(
            (np.array([0] * len(tokenized_examples))[:, None], vectorized_raw_all_examples)).tocsc()
        response_id2paths_dict[id_] = [
            vectorized_raw_all_examples[i].nonzero()[1].tolist() for path_cnt, i in
            enumerate(range(len(response_tweets)))]
        # å°†å¥å­è¡¨ç¤ºç¼–ç æˆç›¸åŒçš„é•¿åº¦
        response_id2paths_dict[id_] = pad_zero(response_id2paths_dict[id_])

        random_tweets = example['tweets']
        for i in random_tweets:
            i = replace_url_with_token(i)
            i = emoji_replace(i)
            i = delete_special_freq(i)
            tokens = list(map(str, tokenizer(i)))
            text = ' '.join(tokens)
            tokenized_examples.append(text)

        vectorized_raw_all_examples = count_vectorizer.transform(tqdm(tokenized_examples))

        vectorized_raw_all_examples = sparse.hstack(
            (np.array([0] * len(tokenized_examples))[:, None], vectorized_raw_all_examples)).tocsc()
        random_id2paths_dict[id_] = [
            vectorized_raw_all_examples[i].nonzero()[1].tolist() for path_cnt, i in
            enumerate(range(len(random_tweets)))]
        # å°†å¥å­è¡¨ç¤ºç¼–ç æˆç›¸åŒçš„é•¿åº¦
        random_id2paths_dict[id_] = pad_zero(random_id2paths_dict[id_])

with open('220913__new_data/gen_test_dict_random.json', 'w') as f:
    json.dump(random_id2paths_dict, f)

with open('220913__new_data/gen_test_dict_response.json', 'w') as f:
    json.dump(response_id2paths_dict, f)
